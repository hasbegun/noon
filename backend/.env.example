# Backend API Configuration
# Copy this file to .env and update the values as needed

# ============================================
# Ollama API Configuration
# ============================================
# For local development (Ollama running on host):
# OLLAMA_API_URL=http://localhost:11434/api/generate

# For Docker container (accessing Ollama on host):
OLLAMA_API_URL=http://host.docker.internal:11434/api/generate

# ============================================
# Llama.cpp API Configuration
# ============================================
# For local development:
# LLAMACPP_API_URL=http://localhost:8088/v1/chat/completions

# For Docker container:
LLAMACPP_API_URL=http://host.docker.internal:8088/v1/chat/completions

# ============================================
# ML Configuration
# ============================================
# ML inference is now integrated directly into the backend
# No separate ML service URL needed
# The backend loads models from the mounted ML directory

# ============================================
# Bloom Filter Configuration
# ============================================
# Size of the bloom filter for duplicate detection
BLOOMFILTER_SIZE=100

# False positive rate for the bloom filter
BLOOMFILTER_FPR=0.000001

# ============================================
# Application Configuration
# ============================================
# FastAPI host (usually 0.0.0.0 in container, 127.0.0.1 for local)
# HOST=0.0.0.0

# FastAPI port
# PORT=8000

# Enable debug mode
# DEBUG=false

# ============================================
# Database Configuration (if using PostgreSQL instead of SQLite)
# ============================================
# DATABASE_URL=postgresql://user:password@localhost:5432/dbname
